{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torchtext.vocab import GloVe\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torchtext.data import get_tokenizer\n",
    "from torchtext.vocab import GloVe\n",
    "# GloVe is well embedded in the torchtext package, easy to use\n",
    "glove = GloVe(name='6B', dim=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [ 0.3668, -0.0684,  0.2483, -0.3375,  0.8622, -0.3554,  0.5974,  0.3588,\n",
      "         -0.0489,  0.2077,  0.5716, -0.3194,  0.2442,  0.1925, -0.2414,  0.0193,\n",
      "          0.2527,  0.2217, -0.3872,  0.1858,  0.0425, -0.1987,  0.5122,  1.0127,\n",
      "          0.2462, -0.1484,  0.3403, -0.4819,  0.2935, -0.2058,  0.1011,  0.5394,\n",
      "         -0.0551, -0.2674, -0.2362,  0.1941,  0.3256,  0.2670, -0.7003, -0.1265,\n",
      "         -0.0400, -0.1269,  0.3574, -0.0255,  0.3822,  0.0805,  0.5139, -1.1916,\n",
      "         -0.3035, -0.9837, -0.3032, -0.2324,  0.3917,  0.6209, -0.2342, -2.4306,\n",
      "         -0.3120,  0.2025,  1.3618,  0.2672, -0.5367,  0.3688, -0.4156, -0.4474,\n",
      "          0.5197, -0.1808, -0.2543,  0.2538,  0.5296,  0.4168,  0.4412,  0.4566,\n",
      "         -0.7514,  0.3024, -0.8125, -0.0619,  0.2169,  0.4603, -1.0819, -0.2741,\n",
      "          0.4917,  0.2430,  0.0773,  0.0366, -0.3204, -0.0267,  0.1401, -0.3633,\n",
      "         -0.2605,  0.4131,  0.0760, -0.0543, -0.1023,  0.6122, -0.2879,  0.3588,\n",
      "         -0.0892,  0.5206,  0.3982, -0.9714],\n",
      "        [ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
      "          0.0000,  0.0000,  0.0000,  0.0000],\n",
      "        [-0.1077,  0.1105,  0.5981, -0.5436,  0.6740,  0.1066,  0.0389,  0.3548,\n",
      "          0.0635, -0.0942,  0.1579, -0.8166,  0.1417,  0.2194,  0.5850, -0.5216,\n",
      "          0.2278, -0.1664, -0.6823,  0.3587,  0.4257,  0.1902,  0.9196,  0.5756,\n",
      "          0.4618,  0.4236, -0.0954, -0.4275, -0.1657, -0.0568, -0.2959,  0.2604,\n",
      "         -0.2661, -0.0704, -0.2766,  0.1582,  0.6982,  0.4308,  0.2795, -0.4544,\n",
      "         -0.3380, -0.5818,  0.2236, -0.5778, -0.2686, -0.2042,  0.5639, -0.5852,\n",
      "         -0.1436, -0.6422,  0.0055, -0.3525,  0.1616,  1.1796, -0.4767, -2.7553,\n",
      "         -0.1321, -0.0477,  1.0655,  1.1034, -0.2208,  0.1867,  0.1318,  0.1512,\n",
      "          0.7131, -0.3521,  0.9135,  0.6178,  0.7099,  0.2395, -0.1457, -0.3786,\n",
      "         -0.0460, -0.4737,  0.2385,  0.2054, -0.1900,  0.3251, -1.1112, -0.3634,\n",
      "          0.9868, -0.0848, -0.5401,  0.1173, -1.0194, -0.2442,  0.1277,  0.0139,\n",
      "          0.0804, -0.3541,  0.3495, -0.7226,  0.3755,  0.4441, -0.9906,  0.6121,\n",
      "         -0.3511, -0.8316,  0.4529,  0.0826],\n",
      "        [ 0.2309,  0.2828,  0.6318, -0.5941, -0.5860,  0.6326,  0.2440, -0.1411,\n",
      "          0.0608, -0.7898, -0.2910,  0.1429,  0.7227,  0.2043,  0.1407,  0.9876,\n",
      "          0.5253,  0.0975,  0.8822,  0.5122,  0.4020,  0.2117, -0.0131, -0.7162,\n",
      "          0.5539,  1.1452, -0.8804, -0.5022, -0.2281,  0.0239,  0.1072,  0.0837,\n",
      "          0.5501,  0.5848,  0.7582,  0.4571, -0.2800,  0.2522,  0.6896, -0.6097,\n",
      "          0.1958,  0.0442, -0.3114, -0.6883, -0.2272,  0.4618, -0.7716,  0.1021,\n",
      "          0.5564,  0.0674, -0.5721,  0.2374,  0.4717,  0.8277, -0.2926, -1.3422,\n",
      "         -0.0993,  0.2814,  0.4160,  0.1058,  0.6220,  0.8950, -0.2345,  0.5135,\n",
      "          0.9938,  1.1846, -0.1636,  0.2065,  0.7385,  0.2406, -0.9647,  0.1348,\n",
      "         -0.0072,  0.3302, -0.1236,  0.2719, -0.4095,  0.0219, -0.6069,  0.4076,\n",
      "          0.1957, -0.4180,  0.1864, -0.0327, -0.7857, -0.1385,  0.0440, -0.0844,\n",
      "          0.0491,  0.2410,  0.4527, -0.1868,  0.4618,  0.0891, -0.1819, -0.0152,\n",
      "         -0.7368, -0.1453,  0.1510, -0.7149]])\n"
     ]
    }
   ],
   "source": [
    "# Get vectors\n",
    "tensor = glove.get_vecs_by_tokens(['', '1998', '199999998', ',', 'cat'], True)\n",
    "print(tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400000\n",
      "the , . of\n"
     ]
    }
   ],
   "source": [
    "myvocab = glove.itos\n",
    "print(len(myvocab))\n",
    "print(myvocab[0], myvocab[1], myvocab[2], myvocab[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "queen\n",
      "short\n",
      "yellow\n"
     ]
    }
   ],
   "source": [
    "def get_counterpart(x1, y1, x2):\n",
    "    \"\"\"Find y2 that makes x1-y1=x2-y2\"\"\"\n",
    "    x1_id = glove.stoi[x1]\n",
    "    y1_id = glove.stoi[y1]\n",
    "    x2_id = glove.stoi[x2]\n",
    "    x1, y1, x2 = glove.get_vecs_by_tokens([x1, y1, x2], True)\n",
    "    target = x2 - x1 + y1\n",
    "    max_sim = 0\n",
    "    max_id = -1\n",
    "    for i in range(len(myvocab)):\n",
    "        vector = glove.get_vecs_by_tokens([myvocab[i]], True)[0]\n",
    "        cossim = torch.dot(target, vector)\n",
    "        if cossim > max_sim and i not in {x1_id, y1_id, x2_id}:\n",
    "            max_sim = cossim\n",
    "            max_id = i\n",
    "    return myvocab[max_id]\n",
    "\n",
    "\n",
    "print(get_counterpart('man', 'woman', 'king'))\n",
    "print(get_counterpart('more', 'less', 'long'))\n",
    "print(get_counterpart('apple', 'red', 'banana'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', ',', 'b']\n"
     ]
    }
   ],
   "source": [
    "from torchtext.data import get_tokenizer\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "print(tokenizer('a, b'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv('/Users/qiaochufeng/Documents/GitHub/assignment-2-text-classification-Carrief-0908/nlp-getting-started/train.csv')\n",
    "test = pd.read_csv('/Users/qiaochufeng/Documents/GitHub/assignment-2-text-classification-Carrief-0908/nlp-getting-started/test.csv')\n",
    "submit = pd.read_csv('/Users/qiaochufeng/Documents/GitHub/assignment-2-text-classification-Carrief-0908/nlp-getting-started/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: (7613, 5)\n",
      "Test size: (3263, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>All residents asked to 'shelter in place' are ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>7</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text  \\\n",
       "0   1     NaN      NaN  Our Deeds are the Reason of this #earthquake M...   \n",
       "1   4     NaN      NaN             Forest fire near La Ronge Sask. Canada   \n",
       "2   5     NaN      NaN  All residents asked to 'shelter in place' are ...   \n",
       "3   6     NaN      NaN  13,000 people receive #wildfires evacuation or...   \n",
       "4   7     NaN      NaN  Just got sent this photo from Ruby #Alaska as ...   \n",
       "\n",
       "   target  \n",
       "0       1  \n",
       "1       1  \n",
       "2       1  \n",
       "3       1  \n",
       "4       1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('Train size:', train.shape)\n",
    "print('Test size:', test.shape)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_lines = train['text'].tolist()\n",
    "test_lines = test['text'].tolist()\n",
    "tgt0_lines = train[(train.target==0)]['text'].tolist()\n",
    "tgt1_lines = train[(train.target==1)]['text'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_DIM = 100\n",
    "GLOVE = GloVe(name='6B', dim=GLOVE_DIM)\n",
    "\n",
    "class twitterDataset_train():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        pos_lines = train[(train.target==1)]['text'].tolist()\n",
    "        neg_lines = train[(train.target==0)]['text'].tolist()\n",
    "        self.lines = pos_lines + neg_lines\n",
    "        self.pos_length = len(pos_lines)\n",
    "        self.neg_length = len(neg_lines)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.pos_length + self.neg_length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        sentence = self.tokenizer(self.lines[index])\n",
    "        x = GLOVE.get_vecs_by_tokens(sentence)\n",
    "        label = 1 if index < self.pos_length else 0\n",
    "        return x, label\n",
    "    \n",
    "class twitterDataset_test():\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.tokenizer = get_tokenizer('basic_english')\n",
    "        self.lines = test['text'].tolist()\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.lines\n",
    "\n",
    "def collate_fn(batch):\n",
    "    x, y = zip(*batch)\n",
    "    x_pad = pad_sequence(x, batch_first=True)\n",
    "    y = torch.Tensor(y)\n",
    "    return x_pad, y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def get_dataloader():\n",
    "    def collate_fn(batch):\n",
    "        x, y = zip(*batch)\n",
    "        x_pad = pad_sequence(x, batch_first=True)\n",
    "        y = torch.Tensor(y)\n",
    "        return x_pad, y\n",
    "\n",
    "    train_dataloader = DataLoader(twitterDataset_train,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate_fn)\n",
    "    test_dataloader = DataLoader(twitterDataset_test,\n",
    "                    batch_size=32,\n",
    "                    shuffle=True,\n",
    "                    collate_fn=collate_fn)\n",
    "    return train_dataloader, test_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'type' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[39], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[39mreturn\u001b[39;00m output\n\u001b[1;32m     20\u001b[0m device \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m---> 21\u001b[0m train_dataloader, test_dataloader \u001b[39m=\u001b[39m get_dataloader()\n\u001b[1;32m     22\u001b[0m model \u001b[39m=\u001b[39m RNN()\u001b[39m.\u001b[39mto(device)\n",
      "Cell \u001b[0;32mIn[38], line 11\u001b[0m, in \u001b[0;36mget_dataloader\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     y \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mTensor(y)\n\u001b[1;32m      9\u001b[0m     \u001b[39mreturn\u001b[39;00m x_pad, y\n\u001b[0;32m---> 11\u001b[0m train_dataloader \u001b[39m=\u001b[39m DataLoader(twitterDataset_train,\n\u001b[1;32m     12\u001b[0m                 batch_size\u001b[39m=\u001b[39;49m\u001b[39m32\u001b[39;49m,\n\u001b[1;32m     13\u001b[0m                 shuffle\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     14\u001b[0m                 collate_fn\u001b[39m=\u001b[39;49mcollate_fn)\n\u001b[1;32m     15\u001b[0m test_dataloader \u001b[39m=\u001b[39m DataLoader(twitterDataset_test,\n\u001b[1;32m     16\u001b[0m                 batch_size\u001b[39m=\u001b[39m\u001b[39m32\u001b[39m,\n\u001b[1;32m     17\u001b[0m                 shuffle\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m     18\u001b[0m                 collate_fn\u001b[39m=\u001b[39mcollate_fn)\n\u001b[1;32m     19\u001b[0m \u001b[39mreturn\u001b[39;00m train_dataloader, test_dataloader\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pt18/lib/python3.9/site-packages/torch/utils/data/dataloader.py:351\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[0;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# map-style\u001b[39;00m\n\u001b[1;32m    350\u001b[0m     \u001b[39mif\u001b[39;00m shuffle:\n\u001b[0;32m--> 351\u001b[0m         sampler \u001b[39m=\u001b[39m RandomSampler(dataset, generator\u001b[39m=\u001b[39;49mgenerator)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n\u001b[1;32m    352\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    353\u001b[0m         sampler \u001b[39m=\u001b[39m SequentialSampler(dataset)  \u001b[39m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pt18/lib/python3.9/site-packages/torch/utils/data/sampler.py:106\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[0;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement, \u001b[39mbool\u001b[39m):\n\u001b[1;32m    103\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mreplacement should be a boolean value, but got \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    104\u001b[0m                     \u001b[39m\"\u001b[39m\u001b[39mreplacement=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mreplacement))\n\u001b[0;32m--> 106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_samples, \u001b[39mint\u001b[39m) \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples \u001b[39m<\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m    107\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39mnum_samples should be a positive integer \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    108\u001b[0m                      \u001b[39m\"\u001b[39m\u001b[39mvalue, but got num_samples=\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_samples))\n",
      "File \u001b[0;32m/opt/homebrew/Caskroom/miniforge/base/envs/pt18/lib/python3.9/site-packages/torch/utils/data/sampler.py:114\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[39m@property\u001b[39m\n\u001b[1;32m    111\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mnum_samples\u001b[39m(\u001b[39mself\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m    112\u001b[0m     \u001b[39m# dataset size might change at runtime\u001b[39;00m\n\u001b[1;32m    113\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m--> 114\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdata_source)\n\u001b[1;32m    115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_samples\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'type' has no len()"
     ]
    }
   ],
   "source": [
    "from torch import nn\n",
    "class RNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_units=64, dropout_rate=0.5):\n",
    "        super().__init__()\n",
    "        self.drop = nn.Dropout(dropout_rate)\n",
    "        self.rnn = nn.GRU(GLOVE_DIM, hidden_units, 1, batch_first=True)\n",
    "        self.linear = nn.Linear(hidden_units, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        # x shape: [batch, max_word_length, embedding_length]\n",
    "        emb = self.drop(x)\n",
    "        output, _ = self.rnn(emb)\n",
    "        output = output[:, -1]\n",
    "        output = self.linear(output)\n",
    "        output = self.sigmoid(output)\n",
    "\n",
    "        return output\n",
    "\n",
    "device = 'cpu'\n",
    "train_dataloader, test_dataloader = get_dataloader()\n",
    "model = RNN().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 交上去的记录 baseline\n",
    "# Epoch 0. loss: 0.6509764790534973\n",
    "# Epoch 1. loss: 0.5327534079551697\n",
    "# Epoch 2. loss: 0.4986315965652466\n",
    "# Epoch 3. loss: 0.47606101632118225\n",
    "# Epoch 4. loss: 0.4756905734539032\n",
    "# Epoch 5. loss: 0.4689613878726959\n",
    "# Epoch 6. loss: 0.46437057852745056\n",
    "# Epoch 7. loss: 0.455255925655365\n",
    "# Epoch 8. loss: 0.457621693611145\n",
    "# Epoch 9. loss: 0.45233598351478577\n",
    "# Epoch 10. loss: 0.4378981590270996\n",
    "# Epoch 11. loss: 0.44445234537124634\n",
    "# Epoch 12. loss: 0.43523845076560974\n",
    "# Epoch 13. loss: 0.42682886123657227\n",
    "# Epoch 14. loss: 0.4274866580963135\n",
    "# Epoch 15. loss: 0.41839489340782166\n",
    "# Epoch 16. loss: 0.4149070382118225\n",
    "# Epoch 17. loss: 0.41745632886886597\n",
    "# Epoch 18. loss: 0.4122700095176697\n",
    "# Epoch 19. loss: 0.41320765018463135\n",
    "# Epoch 20. loss: 0.4069027602672577\n",
    "# Epoch 21. loss: 0.3929371237754822\n",
    "# Epoch 22. loss: 0.40475574135780334\n",
    "# Epoch 23. loss: 0.39060503244400024\n",
    "# Epoch 24. loss: 0.388701468706131\n",
    "# Epoch 25. loss: 0.3816075325012207\n",
    "# Epoch 26. loss: 0.38171839714050293\n",
    "# Epoch 27. loss: 0.3800078332424164\n",
    "# Epoch 28. loss: 0.3772260248661041\n",
    "# Epoch 29. loss: 0.3746449649333954\n",
    "# Epoch 30. loss: 0.3681471645832062\n",
    "# Epoch 31. loss: 0.3671283721923828\n",
    "# Epoch 32. loss: 0.3662094175815582\n",
    "# Epoch 33. loss: 0.3621152341365814\n",
    "# Epoch 34. loss: 0.3589635193347931\n",
    "# Epoch 35. loss: 0.36246246099472046\n",
    "# Epoch 36. loss: 0.3522266745567322\n",
    "# Epoch 37. loss: 0.35584887862205505\n",
    "# Epoch 38. loss: 0.3515584170818329\n",
    "# Epoch 39. loss: 0.3519766628742218\n",
    "# Epoch 40. loss: 0.3430260717868805\n",
    "# Epoch 41. loss: 0.339802086353302\n",
    "# Epoch 42. loss: 0.33826959133148193\n",
    "# Epoch 43. loss: 0.34149956703186035\n",
    "# Epoch 44. loss: 0.3370189964771271\n",
    "# Epoch 45. loss: 0.324999064207077\n",
    "# Epoch 46. loss: 0.32812047004699707\n",
    "# Epoch 47. loss: 0.32268932461738586\n",
    "# Epoch 48. loss: 0.3205323815345764\n",
    "# Epoch 49. loss: 0.315046489238739\n",
    "# Epoch 50. loss: 0.3277100622653961\n",
    "# Epoch 51. loss: 0.3237587511539459\n",
    "# Epoch 52. loss: 0.3185259997844696\n",
    "# Epoch 53. loss: 0.31613194942474365\n",
    "# Epoch 54. loss: 0.3110210597515106\n",
    "# Epoch 55. loss: 0.3070056140422821\n",
    "# Epoch 56. loss: 0.31371772289276123\n",
    "# Epoch 57. loss: 0.30971306562423706\n",
    "# Epoch 58. loss: 0.2946414649486542\n",
    "# Epoch 59. loss: 0.3195744752883911\n",
    "# Epoch 60. loss: 0.3035644590854645\n",
    "# Epoch 61. loss: 0.2950517535209656\n",
    "# Epoch 62. loss: 0.30289679765701294\n",
    "# Epoch 63. loss: 0.29878732562065125\n",
    "# Epoch 64. loss: 0.29898324608802795\n",
    "# Epoch 65. loss: 0.30125465989112854\n",
    "# Epoch 66. loss: 0.29164087772369385\n",
    "# Epoch 67. loss: 0.2961573600769043\n",
    "# Epoch 68. loss: 0.28568440675735474\n",
    "# Epoch 69. loss: 0.2909999191761017\n",
    "# Epoch 70. loss: 0.2963806390762329\n",
    "# Epoch 71. loss: 0.2928559184074402\n",
    "# Epoch 72. loss: 0.2847306728363037\n",
    "# Epoch 73. loss: 0.2902323007583618\n",
    "# Epoch 74. loss: 0.2823370695114136\n",
    "# Epoch 75. loss: 0.2800539433956146\n",
    "# Epoch 76. loss: 0.28621771931648254\n",
    "# Epoch 77. loss: 0.27567166090011597\n",
    "# Epoch 78. loss: 0.27030372619628906\n",
    "# Epoch 79. loss: 0.2739236056804657\n",
    "# Epoch 80. loss: 0.2746753990650177\n",
    "# Epoch 81. loss: 0.2704813778400421\n",
    "# Epoch 82. loss: 0.26951634883880615\n",
    "# Epoch 83. loss: 0.2676199972629547\n",
    "# Epoch 84. loss: 0.2585124969482422\n",
    "# Epoch 85. loss: 0.26896658539772034\n",
    "# Epoch 86. loss: 0.27735984325408936\n",
    "# Epoch 87. loss: 0.2699267864227295\n",
    "# Epoch 88. loss: 0.26850542426109314\n",
    "# Epoch 89. loss: 0.26941996812820435\n",
    "# Epoch 90. loss: 0.26678943634033203\n",
    "# Epoch 91. loss: 0.2646208107471466\n",
    "# Epoch 92. loss: 0.25165173411369324\n",
    "# Epoch 93. loss: 0.2523617446422577\n",
    "# Epoch 94. loss: 0.26316702365875244\n",
    "# Epoch 95. loss: 0.2520638406276703\n",
    "# Epoch 96. loss: 0.24962987005710602\n",
    "# Epoch 97. loss: 0.2640274465084076\n",
    "# Epoch 98. loss: 0.24880437552928925\n",
    "# Epoch 99. loss: 0.25341278314590454\n",
    "# Accuracy: 0.7850262697022767\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pt18",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
