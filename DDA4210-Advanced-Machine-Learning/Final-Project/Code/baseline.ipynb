{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bDI_9hBJOO7b",
        "outputId": "b606d0b3-3415-4484-edab-6357eed39f21"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            ".vector_cache/glove.6B.zip: 862MB [02:40, 5.37MB/s]                           \n",
            "100%|█████████▉| 399999/400000 [00:19<00:00, 20994.12it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0. loss: 0.6931819319725037\n",
            "Epoch 1. loss: 0.6926447153091431\n",
            "Epoch 2. loss: 0.691540539264679\n",
            "Epoch 3. loss: 0.689765453338623\n",
            "Epoch 4. loss: 0.6411824822425842\n",
            "Epoch 5. loss: 0.6022836565971375\n",
            "Epoch 6. loss: 0.593013346195221\n",
            "Epoch 7. loss: 0.5925948619842529\n",
            "Epoch 8. loss: 0.5849927067756653\n",
            "Epoch 9. loss: 0.5718404054641724\n",
            "Accuracy: 0.7588\n"
          ]
        }
      ],
      "source": [
        "from torch import nn\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchtext.data import get_tokenizer\n",
        "from torchtext.vocab import GloVe\n",
        "import torch\n",
        "from torch.nn.utils.rnn import pad_sequence\n",
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "class IMDBDataset(Dataset):\n",
        "    def __init__(self,data,tokenizer):\n",
        "        super().__init__()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.data = data\n",
        "        self.texts = data[\"text\"]\n",
        "        self.labels = data[\"label\"]\n",
        "        self.GLOVE_DIM = 100\n",
        "        self.GLOVE = GloVe(name='6B', dim=self.GLOVE_DIM)\n",
        "        \n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        sentence = self.tokenizer(self.texts[index])\n",
        "        x = self.GLOVE.get_vecs_by_tokens(sentence)\n",
        "        label = self.labels[index]\n",
        "        return x, label\n",
        "\n",
        "\n",
        "def get_dataloader(data,tokenizer):\n",
        "    def collate_fn(batch):\n",
        "        x, y = zip(*batch)\n",
        "        x_pad = pad_sequence(x, batch_first=True)\n",
        "        y = torch.Tensor(y)\n",
        "        return x_pad, y\n",
        "    dataloader = DataLoader(IMDBDataset(data,tokenizer),\n",
        "                    batch_size=32,\n",
        "                    shuffle=True,\n",
        "                    collate_fn=collate_fn)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "\n",
        "class RNN(torch.nn.Module):\n",
        "    def __init__(self, hidden_units=64, dropout_rate=0.5):\n",
        "        super().__init__()\n",
        "        self.drop = nn.Dropout(dropout_rate)\n",
        "        self.GLOVE_DIM = 100\n",
        "        self.rnn = nn.LSTM(self.GLOVE_DIM, hidden_units, 1, batch_first=True)\n",
        "        self.linear = nn.Linear(hidden_units, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        # x shape: [batch, max_word_length, embedding_length]\n",
        "        emb = self.drop(x)\n",
        "        output, _ = self.rnn(emb)\n",
        "        output = output[:, -1]\n",
        "        output = self.linear(output)\n",
        "        output = self.sigmoid(output)\n",
        "\n",
        "        return output\n",
        "\n",
        "device = 'cuda:0'\n",
        "tokenizer = get_tokenizer('basic_english')\n",
        "data_train = pd.read_csv(\"/content/drive/MyDrive/DDA4210project/IMDB_dataset/Train.csv\")\n",
        "data_val = pd.read_csv(\"/content/drive/MyDrive/DDA4210project/IMDB_dataset/Test.csv\")\n",
        "train_dataloader = get_dataloader(data_train,tokenizer)\n",
        "test_dataloader = get_dataloader(data_val,tokenizer)\n",
        "model = RNN().to(device)\n",
        "# train\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "citerion = torch.nn.BCELoss()\n",
        "for epoch in range(10):\n",
        "    loss_sum = 0\n",
        "    dataset_len = len(train_dataloader.dataset)\n",
        "    for x, y in train_dataloader:\n",
        "        batchsize = y.shape[0]\n",
        "        x = x.to(device)\n",
        "        y = y.to(device)\n",
        "        hat_y = model(x)\n",
        "        hat_y = hat_y.squeeze(-1)\n",
        "        loss = citerion(hat_y, y)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
        "        optimizer.step()\n",
        "\n",
        "        loss_sum += loss * batchsize\n",
        "\n",
        "    print(f'Epoch {epoch}. loss: {loss_sum / dataset_len}')\n",
        "\n",
        "torch.save(model.state_dict(), 'rnn.pth')\n",
        "\n",
        "# val\n",
        "\n",
        "# model.load_state_dict(\n",
        "#     torch.load('rnn.pth', 'cuda:0'))\n",
        "\n",
        "accuracy = 0\n",
        "\n",
        "dataset_len = len(test_dataloader.dataset)\n",
        "model.eval()\n",
        "for x, y in test_dataloader:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    with torch.no_grad():\n",
        "        hat_y = model(x)\n",
        "    hat_y.squeeze_(1)\n",
        "    predictions = torch.where(hat_y > 0.5, 1, 0)\n",
        "    score = torch.sum(torch.where(predictions == y, 1, 0))\n",
        "    accuracy += score.item()\n",
        "accuracy /= dataset_len\n",
        "\n",
        "print(f'Accuracy: {accuracy}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# val\n",
        "def get_dataloader(data,tokenizer):\n",
        "    def collate_fn(batch):\n",
        "        x, y = zip(*batch)\n",
        "        x_pad = pad_sequence(x, batch_first=True)\n",
        "        y = torch.Tensor(y)\n",
        "        return x_pad, y\n",
        "    dataloader = DataLoader(IMDBDataset(data,tokenizer),\n",
        "                    batch_size=1,\n",
        "                    shuffle=True,\n",
        "                    collate_fn=collate_fn)\n",
        "\n",
        "    return dataloader\n",
        "\n",
        "model.load_state_dict(\n",
        "     torch.load('rnn.pth', 'cuda:0'))\n",
        "\n",
        "accuracy = 0\n",
        "n = data_val.shape[0]\n",
        "test_dataloader = get_dataloader(data_val,tokenizer)\n",
        "dataset_len = len(test_dataloader.dataset)\n",
        "pre_label = []\n",
        "labels = []\n",
        "model.eval()\n",
        "for x, y in test_dataloader:\n",
        "    x = x.to(device)\n",
        "    y = y.to(device)\n",
        "    with torch.no_grad():\n",
        "        hat_y = model(x)\n",
        "    hat_y.squeeze_(1)\n",
        "    predictions = torch.where(hat_y > 0.5, 1, 0)\n",
        "    score = torch.sum(torch.where(predictions == y, 1, 0))\n",
        "    accuracy += score.item()\n",
        "    pre_label.append(predictions.detach().cpu().numpy()[0])\n",
        "    labels.append(y.detach().cpu().numpy()[0])\n",
        "accuracy /= dataset_len\n",
        "\n",
        "print(f'Accuracy: {accuracy}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HOEDkUX4VH9N",
        "outputId": "99c3bbe5-720d-4709-d5db-8fb342c01342"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.7824\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "report = classification_report(labels, pre_label, digits=4)"
      ],
      "metadata": {
        "id": "fZQGxGZKWT5w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(report)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vlCc5KF1V6Jg",
        "outputId": "ee0b4433-c9de-4ede-9de1-892175d61286"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "         0.0     0.7394    0.8709    0.7998      2495\n",
            "         1.0     0.8438    0.6942    0.7617      2505\n",
            "\n",
            "    accuracy                         0.7824      5000\n",
            "   macro avg     0.7916    0.7826    0.7807      5000\n",
            "weighted avg     0.7917    0.7824    0.7807      5000\n",
            "\n"
          ]
        }
      ]
    }
  ]
}